{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "parent_dir = os.path.abspath(\"./..\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "# LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "# os.environ['LANGSMITH_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGSMITH_PROJECT'] = 'deep-research'\n",
    "# os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "\n",
    "OPENAI_API_KEY=os.getenv('AZURE_OPENAI_API_KEY_US2') \n",
    "os.environ['OPENAI_API_VERSION'] = '2024-12-01-preview'\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = \"https://agents-4on.cognitiveservices.azure.com/\"\n",
    "os.environ['AZURE_OPENAI_DEPLOYMENT'] = \"gpt-4.1-eus2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642581e",
   "metadata": {},
   "source": [
    "### User Clarification and Brief Generation\n",
    "\n",
    "*The goal of scoping is to gather user-context needed for research.*\n",
    "\n",
    "We'll scope the research in two phases:\n",
    "\n",
    "1. **User Clarification** - Determines if additional clarification is needed from the user\n",
    "2. **Brief Generation** - Transforms the conversation into a detailed research brief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_prompt\n",
    "from src.deep_research_from_scratch.prompts import clarify_with_user_instructions\n",
    "show_prompt(clarify_with_user_instructions, \"Clarify with User Instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_prompt\n",
    "from src.deep_research_from_scratch.prompts import transform_messages_into_research_topic_prompt\n",
    "show_prompt(transform_messages_into_research_topic_prompt, \"Research Topic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063982a",
   "metadata": {},
   "source": [
    "#### State and Schemas\n",
    "\n",
    "First, we'll define the state object and schemas for our research process. \n",
    "\n",
    "The state object serves as our primary mechanism for storing and passing context between different phases of the research workflow. \n",
    "\n",
    "We can use it to [write and select context](https://blog.langchain.com/context-engineering-for-agents/) that will be used to guide the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67311e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/deep_research_from_scratch/state_scope.py\n",
    "\n",
    "\"\"\"State Definitions and Pydantic Schemas for Research Scoping.\n",
    "\n",
    "This defines the state objects and structured schemas used for\n",
    "the research agent scoping workflow, including researcher state management and output schemas.\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing_extensions import Optional, Annotated, List, Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# ===== STATE DEFINITIONS =====\n",
    "\n",
    "class AgentInputState(MessagesState):\n",
    "    \"\"\"Input state for the full agent - only contains messages from user input.\"\"\"\n",
    "    pass\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"\n",
    "    Main state for the full multi-agent research system.\n",
    "    \n",
    "    Extends MessagesState with additional fields for research coordination.\n",
    "    Note: Some fields are duplicated across different state classes for proper\n",
    "    state management between subgraphs and the main workflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Research brief generated from user conversation history\n",
    "    research_brief: Optional[str]\n",
    "    # Messages exchanged with the supervisor agent for coordination\n",
    "    supervisor_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # Raw unprocessed research notes collected during the research phase\n",
    "    raw_notes: Annotated[list[str], operator.add] = []\n",
    "    # Processed and structured notes ready for report generation\n",
    "    notes: Annotated[list[str], operator.add] = []\n",
    "    # Final formatted research report\n",
    "    final_report: str\n",
    "\n",
    "# ===== STRUCTURED OUTPUT SCHEMAS =====\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    \"\"\"Schema for user clarification decision and questions.\"\"\"\n",
    "    \n",
    "    need_clarification: bool = Field(\n",
    "        description=\"Whether the user needs to be asked a clarifying question.\",\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"A question to ask the user to clarify the report scope\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description=\"Verify message that we will start research after the user has provided the necessary information.\",\n",
    "    )\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"Schema for structured research brief generation.\"\"\"\n",
    "    \n",
    "    research_brief: str = Field(\n",
    "        description=\"A research question that will be used to guide the research.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15bc91d",
   "metadata": {},
   "source": [
    "#### Scope Research\n",
    "\n",
    "Now, we'll create a simple workflow to clarify the user's intent and write a research brief.\n",
    "\n",
    "We'll let the LLM determine whether it has sufficient clarification to write the brief.\n",
    " \n",
    "This will use LangGraph's [Command](https://langchain-ai.github.io/langgraph/how-tos/graph-api/#combine-control-flow-and-state-updates-with-command) to direct the control flow and updating state. The `Command` object takes two key parameters:\n",
    "- `goto`: Specifies the next node to execute (or `END` to terminate)\n",
    "- `update`: Dictionary of state updates to apply before transitioning\n",
    "\n",
    "This pattern allows our functions to both process data and direct the workflow based on their results. \n",
    "\n",
    "It creates a more flexible and maintainable system than traditional static graph structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90f2dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/deep_research_from_scratch/research_agent_scope.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/deep_research_from_scratch/research_agent_scope.py\n",
    "\n",
    "\"\"\"User Clarification and Research Brief Generation.\n",
    "\n",
    "This module implements the scoping phase of the research workflow, where we:\n",
    "1. Assess if the user's request needs clarification\n",
    "2. Generate a detailed research brief from the conversation\n",
    "\n",
    "The workflow uses structured output to make deterministic decisions about\n",
    "whether sufficient context exists to proceed with research.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, AIMessage, get_buffer_string\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "from src.deep_research_from_scratch.prompts import clarify_with_user_instructions, transform_messages_into_research_topic_prompt\n",
    "from src.deep_research_from_scratch.state_scope import AgentState, ClarifyWithUser, ResearchQuestion, AgentInputState\n",
    "\n",
    "#-Azure-------------------------------------\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "#-------------------------------------------\n",
    "\n",
    "# ===== UTILITY FUNCTIONS =====\n",
    "\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
    "    # return datetime.now().strftime(\"%a %b %-d, %Y\") # Unix\n",
    "    return datetime.now().strftime(\"%a %b %#d, %Y\") # Windows\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "# Initialize model\n",
    "\n",
    "# model = init_chat_model(model=\"openai:gpt-4.1\", temperature=0.0)\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY_US2'),  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    ")\n",
    "\n",
    "# ===== WORKFLOW NODES =====\n",
    "\n",
    "def clarify_with_user(state: AgentState) -> Command[Literal[\"write_research_brief\", \"__end__\"]]:\n",
    "    \"\"\"\n",
    "    Determine if the user's request contains sufficient information to proceed with research.\n",
    "    \n",
    "    Uses structured output to make deterministic decisions and avoid hallucination.\n",
    "    Routes to either research brief generation or ends with a clarification question.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = model.with_structured_output(ClarifyWithUser)\n",
    "\n",
    "    # Invoke the model with clarification instructions\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=clarify_with_user_instructions.format(\n",
    "            messages=get_buffer_string(messages=state[\"messages\"]), \n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Route based on clarification need\n",
    "    if response.need_clarification:\n",
    "        return Command(\n",
    "            goto=END, \n",
    "            update={\"messages\": [AIMessage(content=response.question)]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"write_research_brief\", \n",
    "            update={\"messages\": [AIMessage(content=response.verification)]}\n",
    "        )\n",
    "\n",
    "def write_research_brief(state: AgentState):\n",
    "    \"\"\"\n",
    "    Transform the conversation history into a comprehensive research brief.\n",
    "    \n",
    "    Uses structured output to ensure the brief follows the required format\n",
    "    and contains all necessary details for effective research.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = model.with_structured_output(ResearchQuestion)\n",
    "    \n",
    "    # Generate research brief from conversation history\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=transform_messages_into_research_topic_prompt.format(\n",
    "            messages=get_buffer_string(state.get(\"messages\", [])),\n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # Update state with generated research brief and pass it to the supervisor\n",
    "    return {\n",
    "        \"research_brief\": response.research_brief,\n",
    "        \"supervisor_messages\": [HumanMessage(content=f\"{response.research_brief}.\")]\n",
    "    }\n",
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "# Build the scoping workflow\n",
    "deep_researcher_builder = StateGraph(AgentState, input_schema=AgentInputState)\n",
    "\n",
    "# Add workflow nodes\n",
    "deep_researcher_builder.add_node(\"clarify_with_user\", clarify_with_user)\n",
    "deep_researcher_builder.add_node(\"write_research_brief\", write_research_brief)\n",
    "\n",
    "# Add workflow edges\n",
    "deep_researcher_builder.add_edge(START, \"clarify_with_user\")\n",
    "deep_researcher_builder.add_edge(\"write_research_brief\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "scope_research = deep_researcher_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8c519c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile with in-memory checkpointer to test in notebook\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from src.deep_research_from_scratch.research_agent_scope import deep_researcher_builder\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "scope = deep_researcher_builder.compile(checkpointer=checkpointer)\n",
    "# display(Image(scope.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfe52f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ§‘ Human â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Ğ˜ÑĞºĞ°Ğ¼ Ğ´Ğ° Ñ€Ğ°Ğ·Ğ±ĞµÑ€Ğ° Ğ·Ğ° ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ‚Ğ° Ñ Ğ›ÑƒĞºĞ¾Ğ¹Ğ» Ğ² Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ Ğ² ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° ÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¸Ñ‚Ğµ, Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ Ğ¾Ñ‚ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ‚Ğ° Ğ½Ğ°     <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span> Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚Ğ° Ğ”Ğ¾Ğ½Ğ°Ğ»Ğ´ Ğ¢Ñ€ÑŠĞ¼Ğ¿.                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">â”‚</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34mâ•­â”€\u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34m ğŸ§‘ Human \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m Ğ˜ÑĞºĞ°Ğ¼ Ğ´Ğ° Ñ€Ğ°Ğ·Ğ±ĞµÑ€Ğ° Ğ·Ğ° ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ‚Ğ° Ñ Ğ›ÑƒĞºĞ¾Ğ¹Ğ» Ğ² Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ Ğ² ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° ÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¸Ñ‚Ğµ, Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ Ğ¾Ñ‚ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ‚Ğ° Ğ½Ğ°     \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ”‚\u001b[0m Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚Ğ° Ğ”Ğ¾Ğ½Ğ°Ğ»Ğ´ Ğ¢Ñ€ÑŠĞ¼Ğ¿.                                                                                        \u001b[34mâ”‚\u001b[0m\n",
       "\u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ğŸ“ AI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚</span> Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑÑ‚Ğ°. Ğ Ğ°Ğ·Ğ±Ğ¸Ñ€Ğ°Ğ¼, Ñ‡Ğµ Ğ¶ĞµĞ»Ğ°ĞµÑ‚Ğµ Ğ´Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ‚Ğ½Ğ¾ÑĞ½Ğ¾ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ‚Ğ° Ñ Ğ›ÑƒĞºĞ¾Ğ¹Ğ» Ğ² Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ Ğ²ÑŠĞ²   <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚</span> Ğ²Ñ€ÑŠĞ·ĞºĞ° ÑÑŠÑ ÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¸Ñ‚Ğµ, Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ Ğ¾Ñ‚ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚Ğ° Ğ”Ğ¾Ğ½Ğ°Ğ»Ğ´ Ğ¢Ñ€ÑŠĞ¼Ğ¿. Ğ—Ğ°Ğ¿Ğ¾Ñ‡Ğ²Ğ°Ğ¼ Ğ¿Ñ€Ğ¾ÑƒÑ‡Ğ²Ğ°Ğ½ĞµÑ‚Ğ¾ Ğ¿Ğ¾ Ñ‚Ğ°Ğ·Ğ¸      <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚</span> Ñ‚ĞµĞ¼Ğ°.                                                                                                           <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â”‚</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mâ•­â”€\u001b[0m\u001b[37mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[37m ğŸ“ AI \u001b[0m\u001b[37mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[37mâ”€â•®\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m Ğ‘Ğ»Ğ°Ğ³Ğ¾Ğ´Ğ°Ñ€Ñ Ğ·Ğ° Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ÑÑ‚Ğ°. Ğ Ğ°Ğ·Ğ±Ğ¸Ñ€Ğ°Ğ¼, Ñ‡Ğµ Ğ¶ĞµĞ»Ğ°ĞµÑ‚Ğµ Ğ´Ğ° Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚Ğµ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ¾Ñ‚Ğ½Ğ¾ÑĞ½Ğ¾ ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ‚Ğ° Ñ Ğ›ÑƒĞºĞ¾Ğ¹Ğ» Ğ² Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ Ğ²ÑŠĞ²   \u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m Ğ²Ñ€ÑŠĞ·ĞºĞ° ÑÑŠÑ ÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¸Ñ‚Ğµ, Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ Ğ¾Ñ‚ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚Ğ° Ğ”Ğ¾Ğ½Ğ°Ğ»Ğ´ Ğ¢Ñ€ÑŠĞ¼Ğ¿. Ğ—Ğ°Ğ¿Ğ¾Ñ‡Ğ²Ğ°Ğ¼ Ğ¿Ñ€Ğ¾ÑƒÑ‡Ğ²Ğ°Ğ½ĞµÑ‚Ğ¾ Ğ¿Ğ¾ Ñ‚Ğ°Ğ·Ğ¸      \u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ”‚\u001b[0m Ñ‚ĞµĞ¼Ğ°.                                                                                                           \u001b[37mâ”‚\u001b[0m\n",
       "\u001b[37mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the workflow\n",
    "from utils import format_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# q = 'I want to research the best coffee shops in San Francisco.'        \n",
    "# q = 'I am researchin how to develop a multiagent application with sub-agents for RAG, querying an SQL database, and websearch, and a main agent that compliles all the info a final report. I intend to use Python and LangGraph'\n",
    "q = 'Ğ˜ÑĞºĞ°Ğ¼ Ğ´Ğ° Ñ€Ğ°Ğ·Ğ±ĞµÑ€Ğ° Ğ·Ğ° ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸ÑÑ‚Ğ° Ñ Ğ›ÑƒĞºĞ¾Ğ¹Ğ» Ğ² Ğ‘ÑŠĞ»Ğ³Ğ°Ñ€Ğ¸Ñ Ğ² ÑĞ»ĞµĞ´ÑÑ‚Ğ²Ğ¸Ğµ Ğ½Ğ° ÑĞ°Ğ½ĞºÑ†Ğ¸Ğ¸Ñ‚Ğµ, Ğ½Ğ°Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸ Ğ¾Ñ‚ Ğ°Ğ´Ğ¼Ğ¸Ğ½Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸ÑÑ‚Ğ° Ğ½Ğ° Ğ¿Ñ€ĞµĞ·Ğ¸Ğ´ĞµĞ½Ñ‚Ğ° Ğ”Ğ¾Ğ½Ğ°Ğ»Ğ´ Ğ¢Ñ€ÑŠĞ¼Ğ¿.'   \n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result = scope.invoke({\"messages\": [HumanMessage(content= q )]}, config=thread)\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3054f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the workflow\n",
    "# from utils import format_messages\n",
    "# from langchain_core.messages import HumanMessage\n",
    "\n",
    "# q = \"Let's examine coffee quality to assess the best coffee shops in San Francisco\"\n",
    "\n",
    "# thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# result = scope.invoke({\"messages\": [HumanMessage(content= q )]}, config=thread)\n",
    "# format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7247f581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I want to understand the situation regarding Lukoil in Bulgaria in the aftermath of sanctions imposed by the       \n",
       "administration of President Donald Trump. Specifically, I would like a detailed analysis of how these US sanctions \n",
       "affected Lukoil's operations, market position, and political or economic relations in Bulgaria. Please include     \n",
       "details about the timing and scope of the sanctions, any government or corporate responses in Bulgaria, observed   \n",
       "economic or logistical impacts on Lukoil's activities, and notable shifts in regulatory or public discourse in     \n",
       "Bulgaria resulting from these sanctions. Unless specified by the available data, consider various periods and all  \n",
       "types of sanctions imposed by the Trump administration that could have impacted Lukoil in Bulgaria. Prioritize     \n",
       "reputable sources, including Bulgarian government statements, Lukoil corporate communications, and major           \n",
       "international news agencies, with a preference for sources in Bulgarian where available.                           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "I want to understand the situation regarding Lukoil in Bulgaria in the aftermath of sanctions imposed by the       \n",
       "administration of President Donald Trump. Specifically, I would like a detailed analysis of how these US sanctions \n",
       "affected Lukoil's operations, market position, and political or economic relations in Bulgaria. Please include     \n",
       "details about the timing and scope of the sanctions, any government or corporate responses in Bulgaria, observed   \n",
       "economic or logistical impacts on Lukoil's activities, and notable shifts in regulatory or public discourse in     \n",
       "Bulgaria resulting from these sanctions. Unless specified by the available data, consider various periods and all  \n",
       "types of sanctions imposed by the Trump administration that could have impacted Lukoil in Bulgaria. Prioritize     \n",
       "reputable sources, including Bulgarian government statements, Lukoil corporate communications, and major           \n",
       "international news agencies, with a preference for sources in Bulgarian where available.                           \n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(result[\"research_brief\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e8f58d",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now that we've scoped our research, let's test it with a few examples to make sure that it's working as expected.\n",
    "\n",
    "Let's think about what makes for a good research brief:\n",
    "\n",
    "* It captures relevant criteria from the user chat\n",
    "* It does not invent or assume any criteria that the user did not explicitly provide\n",
    "\n",
    "Here are two sample input conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a084c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "conversation_1 = [\n",
    "    HumanMessage(content=\"What's the best way to invest $50,000 for retirement?\"),\n",
    "    AIMessage(content=\"Could you please provide some additional information to tailor the investment advice for your $50,000 retirement goal? Specifically:\\n Your current age or desired retirement age\\n Your risk tolerance (low, medium, high)\\n Any preferences for investment types (e.g., stocks, bonds, mutual funds, real estate)\\n Whether you are investing through a tax-advantaged account (e.g., IRA, 401(k)) or a regular brokerage account\\n This will help me provide more personalized and relevant suggestions.\"),\n",
    "    HumanMessage(content=\"I'm 25 and I want to retire by 45. My risk tolerance is high right now but I think will decrease over time. I have heard that stocks and ETFs are a good choice, but I'm open to anything. And I already have a 401k, but this would just be through a regular brokerage account.\"),\n",
    "]\n",
    "\n",
    "conversation_2 = [\n",
    "    HumanMessage(content=\"I am looking for an apartment in NYC, can you help me?\"),\n",
    "    AIMessage(content=\"Could you please specify your apartment preferences? For example:\\n Desired neighborhoods or boroughs\\n Number of bedrooms/bathrooms\\n Budget range (monthly rent)\\n Any amenities or must-have features\\n Preferred move-in date\\n This information will help me provide the most relevant apartment options in NYC.\"),\n",
    "    HumanMessage(content=\"I'd prefer to live in Chelsea, Flatiron, or West Village. I'm looking for a 2 bed 2 bath, and I am looking for monthly rent below 7k. I'd like this to be a doorman building and have an in unit washer and dryer, but it's okay if there's no washer dryer. It's a plus if the building has a gym. And I'd like to move in in September 2025.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3e351",
   "metadata": {},
   "source": [
    "Now let's manually write out each criteria from these conversations that we would want preserved by a research brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c697e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_1 = [\n",
    "    \"Current age is 25\",\n",
    "    \"Desired retirement age is 45\",\n",
    "    \"Current risk tolerance is high\",\n",
    "    \"Interested in investing in stocks and ETFs\",\n",
    "    \"Open to forms of investment beyond stocks and ETFs\"\n",
    "    \"Investment account is a regular brokerage account\",\n",
    "]\n",
    "\n",
    "criteria_2 = [\n",
    "    \"Looking for a 2 bed 2 bath apartment in Chelsea, Flatiron, or West Village\",\n",
    "    \"Monthly rent below 7k\",\n",
    "    \"Should be in a doorman building\",\n",
    "    \"Ideally have an in unit washer and dryer but not strict\",\n",
    "    \"Ideally have a gym but not strict\",\n",
    "    \"Move in date is September 2025\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01697b45",
   "metadata": {},
   "source": [
    "We're going to use LangSmith to run this experiment. \n",
    "\n",
    "Running an experiment in LangSmith comprises of three steps: \n",
    "\n",
    "1. Creating the dataset\n",
    "2. Writing the evaluator(s)\n",
    "3. Running the experiment\n",
    "\n",
    "We'll start by creating our dataset and adding our two examples to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "\n",
    "# Initialize the LangSmith client\n",
    "# langsmith_client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "langsmith_client = Client(api_key=LANGSMITH_API_KEY)\n",
    "\n",
    "# Create the dataset\n",
    "dataset_name = \"deep_research_scoping\"\n",
    "if not langsmith_client.has_dataset(dataset_name=dataset_name):\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = langsmith_client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"A dataset that measures the quality of research briefs generated from an input conversation\",\n",
    "    )\n",
    "\n",
    "    # Add the examples to the dataset\n",
    "    langsmith_client.create_examples(\n",
    "        dataset_id=dataset.id,\n",
    "        examples=[\n",
    "            {\n",
    "                \"inputs\": {\"messages\": conversation_1},\n",
    "                \"outputs\": {\"criteria\": criteria_1},\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\"messages\": conversation_2},\n",
    "                \"outputs\": {\"criteria\": criteria_2},\n",
    "            },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cefe57",
   "metadata": {},
   "source": [
    "Now, we need to write an evaluator that will compare our research brief against the success criteria that we have specified for each example. For this, we'll use an LLM-as-judge. You can fine some useful tips for writing llm-as-judge evaluators [here](https://hamel.dev/blog/posts/llm-judge/index.html), which include:\n",
    "\n",
    "1. **Role Definition with Expertise Context**\n",
    "   - Defined specific expert roles (\"research brief evaluator\", \"meticulous auditor\")  \n",
    "   - Specialized the role to the specific evaluation domain\n",
    "\n",
    "2. **Clear Task Specification**\n",
    "   - Binary pass/fail judgments (avoiding complex multi-dimensional scoring)\n",
    "   - Explicit task boundaries and objectives\n",
    "   - Focus on actionable evaluation criteria\n",
    "\n",
    "3. **Rich Contextual Background**\n",
    "   - Provide domain-specific context about research brief quality\n",
    "   - Explain the importance of accurate evaluation\n",
    "   - Connect evaluation outcomes to downstream consequences\n",
    "\n",
    "4. **Structured XML Organization**\n",
    "   - Used semantic XML tags for different sections\n",
    "   - Clear separation of role, task, context, inputs, guidelines, and outputs\n",
    "   - Improved prompt parsing and comprehension\n",
    "\n",
    "5. **Comprehensive Guidelines with Examples**\n",
    "   - Detailed PASS/FAIL criteria with specific conditions\n",
    "   - Multiple concrete examples showing correct judgments\n",
    "   - 3-4 examples per prompt covering different scenarios\n",
    "   - Both positive and negative examples for each judgment type\n",
    "   - Edge case handling and decision boundary clarification\n",
    "\n",
    "6. **Explicit Output Instructions**\n",
    "   - Clear guidance on how to apply the evaluation criteria\n",
    "   - Instructions for handling ambiguous cases\n",
    "   - Emphasis on consistency and systematic evaluation\n",
    "\n",
    "7. **Bias Reduction Techniques**\n",
    "   - \"Strict but fair\" guidance to balance precision and recall\n",
    "   - \"When in doubt, lean toward FAIL\" for conservative evaluation\n",
    "   - Systematic evaluation process to reduce subjective variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90155e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.deep_research_from_scratch.prompts import BRIEF_CRITERIA_PROMPT\n",
    "show_prompt(BRIEF_CRITERIA_PROMPT, \"BRIEF_CRITERIA_PROMPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31732ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import cast\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Criteria(BaseModel):\n",
    "    \"\"\"\n",
    "    Individual success criteria evaluation result.\n",
    "    \n",
    "    This model represents a single evaluation criteria that should be present\n",
    "    in the research brief, along with a detailed assessment of whether it was\n",
    "    successfully captured and the reasoning behind that assessment.\n",
    "    \"\"\"\n",
    "    criteria_text: str = Field(\n",
    "        description=\"The specific success criteria being evaluated (e.g., 'Current age is 25', 'Monthly rent below 7k')\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Detailed explanation of why this criteria is or isn't captured in the research brief, including specific evidence from the brief\"\n",
    "    )\n",
    "    is_captured: bool = Field(\n",
    "        description=\"Whether this specific criteria is adequately captured in the research brief (True) or missing/inadequately addressed (False)\"\n",
    "    )\n",
    "\n",
    "def evaluate_success_criteria(outputs: dict, reference_outputs: dict):\n",
    "    \"\"\"\n",
    "    Evaluate whether the research brief captures all required success criteria.\n",
    "    \n",
    "    This function evaluates each criterion individually to provide focused assessment\n",
    "    and detailed reasoning for each evaluation decision.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Dictionary containing the research brief to evaluate\n",
    "        reference_outputs: Dictionary containing the list of success criteria\n",
    "        \n",
    "    Returns:\n",
    "        Dict with evaluation results including score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    research_brief = outputs[\"research_brief\"]\n",
    "    success_criteria = reference_outputs[\"criteria\"]\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    structured_output_model = model.with_structured_output(Criteria)\n",
    "    \n",
    "    # Run evals\n",
    "    responses = structured_output_model.batch([\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=BRIEF_CRITERIA_PROMPT.format(\n",
    "                research_brief=research_brief,\n",
    "                criterion=criterion\n",
    "            )\n",
    "        )\n",
    "    ] \n",
    "    for criterion in success_criteria])\n",
    "    \n",
    "    # Ensure the criteria_text field is populated correctly\n",
    "    individual_evaluations = [\n",
    "        Criteria(\n",
    "            reasoning=response.reasoning,\n",
    "            criteria_text=criterion,\n",
    "            is_captured=response.is_captured\n",
    "        )\n",
    "        for criterion, response in zip(success_criteria, responses)\n",
    "    ]\n",
    "    \n",
    "    # Calculate overall score as percentage of captured criteria\n",
    "    captured_count = sum(1 for eval_result in individual_evaluations if eval_result.is_captured)\n",
    "    total_count = len(individual_evaluations)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"success_criteria_score\", \n",
    "        \"score\": captured_count / total_count if total_count > 0 else 0.0,\n",
    "        \"individual_evaluations\": [\n",
    "            {\n",
    "                \"criteria\": eval_result.criteria_text,\n",
    "                \"captured\": eval_result.is_captured,\n",
    "                \"reasoning\": eval_result.reasoning\n",
    "            }\n",
    "            for eval_result in individual_evaluations\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0ce09",
   "metadata": {},
   "source": [
    "Our second evaluator will check that the research brief does not make any assumptions that the user did not specify in the research brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.deep_research_from_scratch.prompts import BRIEF_HALLUCINATION_PROMPT\n",
    "show_prompt(BRIEF_HALLUCINATION_PROMPT, \"BRIEF_HALLUCINATION_PROMPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89549b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved NoAssumptions class with reasoning field and enhanced descriptions\n",
    "class NoAssumptions(BaseModel):\n",
    "    \"\"\"\n",
    "    Evaluation model for checking if research brief makes unwarranted assumptions.\n",
    "    \n",
    "    This model evaluates whether the research brief contains any assumptions,\n",
    "    inferences, or additions that were not explicitly stated by the user in their\n",
    "    original conversation. It provides detailed reasoning for the evaluation decision.\n",
    "    \"\"\"\n",
    "    no_assumptions: bool = Field(\n",
    "        description=\"Whether the research brief avoids making unwarranted assumptions. True if the brief only includes information explicitly provided by the user, False if it makes assumptions beyond what was stated.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Detailed explanation of the evaluation decision, including specific examples of any assumptions found or confirmation that no assumptions were made beyond the user's explicit statements.\"\n",
    "    )\n",
    "\n",
    "def evaluate_no_assumptions(outputs: dict, reference_outputs: dict):\n",
    "    \"\"\"\n",
    "    Evaluate whether the research brief avoids making unwarranted assumptions.\n",
    "    \n",
    "    This evaluator checks that the research brief only includes information\n",
    "    and requirements that were explicitly provided by the user, without\n",
    "    making assumptions about unstated preferences or requirements.\n",
    "    \n",
    "    Args:\n",
    "        outputs: Dictionary containing the research brief to evaluate\n",
    "        reference_outputs: Dictionary containing the success criteria for reference\n",
    "        \n",
    "    Returns:\n",
    "        Dict with evaluation results including boolean score and detailed reasoning\n",
    "    \"\"\"\n",
    "    research_brief = outputs[\"research_brief\"]\n",
    "    success_criteria = reference_outputs[\"criteria\"]\n",
    "    \n",
    "    model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    structured_output_model = model.with_structured_output(NoAssumptions)\n",
    "    \n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=BRIEF_HALLUCINATION_PROMPT.format(\n",
    "            research_brief=research_brief, \n",
    "            success_criteria=str(success_criteria)\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"no_assumptions_score\", \n",
    "        \"score\": response.no_assumptions,\n",
    "        \"reasoning\": response.reasoning\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6358a2",
   "metadata": {},
   "source": [
    "Now that we have our evaluators, we can run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e589dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def target_func(inputs: dict):\n",
    "    config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "    return scope.invoke(inputs, config=config)\n",
    "\n",
    "langsmith_client.evaluate(\n",
    "    target_func,\n",
    "    data=dataset_name,\n",
    "    evaluators=[evaluate_success_criteria, evaluate_no_assumptions],\n",
    "    experiment_prefix=\"Deep Research Scoping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f64803",
   "metadata": {},
   "source": [
    "Why perform evals like this?\n",
    "\n",
    "* Ensure that individual steps in your application are doing what you expect.\n",
    "* With tracing to LangSmith, you also get a sense of how long each step takes as well as the cost.\n",
    "* You can always try out cheaper, faster models to see if they can get the job done.\n",
    "\n",
    "If your agent makes mistakes, you can tune the prompts that the agent is provided to try and improve performance as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748a850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".agents-ud (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
