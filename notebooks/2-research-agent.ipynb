{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "parent_dir = os.path.abspath(\"./..\")\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "_=load_dotenv(find_dotenv())\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# OPENAI_API_KEY=os.getenv('AZURE_OPENAI_API_KEY_US')\n",
    "# os.environ['OPENAI_API_TYPE'] = 'azure'\n",
    "os.environ['OPENAI_API_VERSION'] = '2024-08-01-preview'\n",
    "os.environ['AZURE_OPENAI_ENDPOINT'] = 'https://azure-chat-try-2.openai.azure.com/'\n",
    "os.environ['AZURE_OPENAI_DEPLOYMENT'] = 'chat-endpoint-us-gpt4o'\n",
    "\n",
    "# LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "# os.environ['LANGSMITH_TRACING_V2'] = 'true'\n",
    "# os.environ['LANGSMITH_PROJECT'] = 'deep-research'\n",
    "# os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Agent \n",
    "\n",
    "*The goal of research is to gather the context requested by the research brief.*\n",
    "\n",
    "[Agents](https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent) are well suited to research because they can flexibly apply different strategies, using intermediate results to guide their exploration.\n",
    "\n",
    "The agent follows a [simple yet effective pattern](https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent):\n",
    "\n",
    "1. **LLM Decision Node**: Analyzes the current state and decides whether to make tool calls or provide a final response\n",
    "2. **Tool Execution Node**: Executes search tools when the LLM determines more information is needed\n",
    "3. **Research Compression Node**: Summarizes and compresses research findings for efficient processing\n",
    "4. **Routing Logic**: Determines workflow continuation based on LLM decisions\n",
    "\n",
    "\n",
    "### Prompt\n",
    "\n",
    "First, we'll define a prompt that instructs our agent to use available search tools. \n",
    "\n",
    "To prevent excessive tool calls and maintain research focus, we use [a few prompting techniques for agents](https://youtu.be/XSZP9GhhuAc?si=zowpViL-2j-vI9hA):\n",
    "\n",
    "#### 1. Think Like The Agent\n",
    "What instructions would you give a new work colleague?\n",
    "- **Read the question carefully** - What specific information does the user need?\n",
    "- **Start with broader searches** - Use broad, comprehensive queries first\n",
    "- **After each search, pause and assess** - Do I have enough to answer? What's still missing?\n",
    "- **Execute narrower searches as you gather information** - Fill in the gaps.\n",
    "\n",
    "#### 2. Concrete Heuristics (Prevent \"Spin-Out\" on excessive tool calls)\n",
    "Use **Hard Limits** to prevent the research agent from calling tools excessively:\n",
    "- **Stop when you can answer confidently** - Don't keep searching for perfection.\n",
    "- **Give it budgets** - Use 2-3 search tool calls for simple queries. Use up to 5 for complex queries.\n",
    "- **Limit** - Always stop after 5 search tool calls if you cannot find the right source(s).\n",
    "\n",
    "#### 3. Show your thinking\n",
    "After each search tool calling, use [`think_tool` to analyze the results](https://www.anthropic.com/engineering/claude-think-tool):\n",
    "- What key information did I find? \n",
    "- What's missing?\n",
    "- Do I have enough to answer the question comprehensively?\n",
    "- Should I search more or provide my answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭────────────────────────────────────────── </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Research Agent Instructions</span><span style=\"color: #000080; text-decoration-color: #000080\"> ──────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You are a research assistant conducting research on the user's input topic. For context, today's date is       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  {date}.                                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;Task&gt;</span>                                                                                                         <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  Your job is to use tools to gather information about the user's input topic.                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You can use any of the tools provided to you to find resources that can help answer the research question.     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;/Task&gt;</span>                                                                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;Available Tools&gt;</span>                                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  You have access to two main tools:                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  1. **tavily_search**: For conducting web searches to gather information                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  2. **think_tool**: For reflection and strategic planning during research                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;/Available Tools&gt;</span>                                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;Instructions&gt;</span>                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  Think like a human researcher with limited time. Follow these steps:                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  1. **Read the question carefully** - What specific information does the user need?                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  2. **Start with broader searches** - Use broad, comprehensive queries first                                    <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  4. **Execute narrower searches as you gather information** - Fill in the gaps                                  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  5. **Stop when you can answer confidently** - Don't keep searching for perfection                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;/Instructions&gt;</span>                                                                                                <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;Hard Limits&gt;</span>                                                                                                  <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **Tool Call Budgets** (Prevent excessive searching):                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Simple queries**: Use 2-3 search tool calls maximum                                                        <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Complex queries**: Use up to 5 search tool calls maximum                                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - **Always stop**: After 5 search tool calls if you cannot find the right sources                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  **Stop Immediately When**:                                                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - You can answer the user's question comprehensively                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - You have 3+ relevant examples/sources for the question                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Your last 2 searches returned similar information                                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;/Hard Limits&gt;</span>                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;Show Your Thinking&gt;</span>                                                                                           <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  After each search tool call, use think_tool to analyze the results:                                            <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - What key information did I find?                                                                             <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - What's missing?                                                                                              <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Do I have enough to answer the question comprehensively?                                                     <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  - Should I search more or provide my answer?                                                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>  <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">&lt;/Show Your Thinking&gt;</span>                                                                                          <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span>                                                                                                                 <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m─────────────────────────────────────────\u001b[0m\u001b[34m \u001b[0m\u001b[1;32mResearch Agent Instructions\u001b[0m\u001b[34m \u001b[0m\u001b[34m─────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You are a research assistant conducting research on the user's input topic. For context, today's date is       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  {date}.                                                                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m<Task>\u001b[0m                                                                                                         \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  Your job is to use tools to gather information about the user's input topic.                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You can use any of the tools provided to you to find resources that can help answer the research question.     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You can call these tools in series or in parallel, your research is conducted in a tool-calling loop.          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m</Task>\u001b[0m                                                                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m<Available Tools>\u001b[0m                                                                                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  You have access to two main tools:                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  1. **tavily_search**: For conducting web searches to gather information                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  2. **think_tool**: For reflection and strategic planning during research                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **CRITICAL: Use think_tool after each search to reflect on results and plan next steps**                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m</Available Tools>\u001b[0m                                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m<Instructions>\u001b[0m                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  Think like a human researcher with limited time. Follow these steps:                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  1. **Read the question carefully** - What specific information does the user need?                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  2. **Start with broader searches** - Use broad, comprehensive queries first                                    \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  3. **After each search, pause and assess** - Do I have enough to answer? What's still missing?                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  4. **Execute narrower searches as you gather information** - Fill in the gaps                                  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  5. **Stop when you can answer confidently** - Don't keep searching for perfection                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m</Instructions>\u001b[0m                                                                                                \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m<Hard Limits>\u001b[0m                                                                                                  \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **Tool Call Budgets** (Prevent excessive searching):                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Simple queries**: Use 2-3 search tool calls maximum                                                        \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Complex queries**: Use up to 5 search tool calls maximum                                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - **Always stop**: After 5 search tool calls if you cannot find the right sources                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  **Stop Immediately When**:                                                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - You can answer the user's question comprehensively                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - You have 3+ relevant examples/sources for the question                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Your last 2 searches returned similar information                                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m</Hard Limits>\u001b[0m                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m<Show Your Thinking>\u001b[0m                                                                                           \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  After each search tool call, use think_tool to analyze the results:                                            \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - What key information did I find?                                                                             \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - What's missing?                                                                                              \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Do I have enough to answer the question comprehensively?                                                     \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  - Should I search more or provide my answer?                                                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m  \u001b[1;33m</Show Your Thinking>\u001b[0m                                                                                          \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m│\u001b[0m                                                                                                                 \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import show_prompt\n",
    "from src.deep_research_from_scratch.prompts import research_agent_prompt\n",
    "show_prompt(research_agent_prompt, \"Research Agent Instructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State and Schemas\n",
    "\n",
    "We'll create the agent state object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/deep_research_from_scratch/state_research.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/deep_research_from_scratch/state_research.py\n",
    "\n",
    "\"\"\"\n",
    "State Definitions and Pydantic Schemas for Research Agent\n",
    "\n",
    "This module defines the state objects and structured schemas used for\n",
    "the research agent workflow, including researcher state management and output schemas.\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing_extensions import TypedDict, Annotated, List, Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "# ===== STATE DEFINITIONS =====\n",
    "\n",
    "class ResearcherState(TypedDict):\n",
    "    \"\"\"\n",
    "    State for the research agent containing message history and research metadata.\n",
    "    \n",
    "    This state tracks the researcher's conversation, iteration count for limiting\n",
    "    tool calls, the research topic being investigated, compressed findings,\n",
    "    and raw research notes for detailed analysis.\n",
    "    \"\"\"\n",
    "    researcher_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    tool_call_iterations: int\n",
    "    research_topic: str\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[List[str], operator.add]\n",
    "\n",
    "class ResearcherOutputState(TypedDict):\n",
    "    \"\"\"\n",
    "    Output state for the research agent containing final research results.\n",
    "    \n",
    "    This represents the final output of the research process with compressed\n",
    "    research findings and all raw notes from the research process.\n",
    "    \"\"\"\n",
    "    compressed_research: str\n",
    "    raw_notes: Annotated[List[str], operator.add]\n",
    "    researcher_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# ===== STRUCTURED OUTPUT SCHEMAS =====\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    \"\"\"Schema for user clarification decisions during scoping phase.\"\"\"\n",
    "    need_clarification: bool = Field(\n",
    "        description=\"Whether the user needs to be asked a clarifying question.\",\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"A question to ask the user to clarify the report scope\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description=\"Verify message that we will start research after the user has provided the necessary information.\",\n",
    "    )\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    \"\"\"Schema for research brief generation.\"\"\"\n",
    "    research_brief: str = Field(\n",
    "        description=\"A research question that will be used to guide the research.\",\n",
    "    )\n",
    "\n",
    "class Summary(BaseModel):\n",
    "    \"\"\"Schema for webpage content summarization.\"\"\"\n",
    "    summary: str = Field(description=\"Concise summary of the webpage content\")\n",
    "    key_excerpts: str = Field(description=\"Important quotes and excerpts from the content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll define our research tool.  \n",
    "\n",
    "#### Tavily SDK Integration\n",
    "\n",
    "We'll use the [Tavily SDK](https://docs.tavily.com/sdk/python/reference) to perform web searches.\n",
    " \n",
    "####  LangChain Tools\n",
    "\n",
    "We'll convert our Tavily search function into a [LangChain tool](https://python.langchain.com/docs/concepts/tools/) using the `@tool` decorator. This enables:\n",
    "- Automatic schema generation for the LLM\n",
    "- Integration with LangChain's agent framework\n",
    "- Standardized tool calling interface\n",
    "- Type validation and documentation\n",
    "\n",
    "Note that some of the arguments are noted as [InjectedToolArg](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.base.InjectedToolArg.html). These are not included in the tool schema sent to the LLM, and instead are injected during execution. This gives us the flexibility to inject values at runtime, but does not burden the language model with the responsibility of managing these arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/deep_research_from_scratch/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/deep_research_from_scratch/utils.py\n",
    "\n",
    "\"\"\"Research Utilities and Tools.\n",
    "\n",
    "This module provides search and content processing utilities for the research agent,\n",
    "including web search capabilities and content summarization tools.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing_extensions import Annotated, List, Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model \n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import tool, InjectedToolArg\n",
    "from tavily import TavilyClient\n",
    "\n",
    "from src.deep_research_from_scratch.state_research import Summary\n",
    "from src.deep_research_from_scratch.prompts import summarize_webpage_prompt\n",
    "\n",
    "#-Azure-------------------------------------\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "#-------------------------------------------\n",
    "\n",
    "# ===== UTILITY FUNCTIONS =====\n",
    "\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %-d, %Y\")\n",
    "\n",
    "def get_current_dir() -> Path:\n",
    "    \"\"\"Get the current directory of the module.\n",
    "\n",
    "    This function is compatible with Jupyter notebooks and regular Python scripts.\n",
    "\n",
    "    Returns:\n",
    "        Path object representing the current directory\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return Path(__file__).resolve().parent\n",
    "    except NameError:  # __file__ is not defined\n",
    "        return Path.cwd()\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "#-Azure------------------------------------------------\n",
    "summarization_model = AzureChatOpenAI(\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY_US'),  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    ")\n",
    "#-----------------------------------------------------\n",
    "\n",
    "# summarization_model = init_chat_model(model=\"openai:gpt-4.1-mini\")\n",
    "tavily_client = TavilyClient()\n",
    "\n",
    "# ===== SEARCH FUNCTIONS =====\n",
    "\n",
    "def tavily_search_multiple(\n",
    "    search_queries: List[str], \n",
    "    max_results: int = 3, \n",
    "    topic: Literal[\"general\", \"news\", \"finance\"] = \"general\", \n",
    "    include_raw_content: bool = True, \n",
    ") -> List[dict]:\n",
    "    \"\"\"Perform search using Tavily API for multiple queries.\n",
    "\n",
    "    Args:\n",
    "        search_queries: List of search queries to execute\n",
    "        max_results: Maximum number of results per query\n",
    "        topic: Topic filter for search results\n",
    "        include_raw_content: Whether to include raw webpage content\n",
    "\n",
    "    Returns:\n",
    "        List of search result dictionaries\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute searches sequentially. Note: yon can use AsyncTavilyClient to parallelize this step.\n",
    "    search_docs = []\n",
    "    for query in search_queries:\n",
    "        result = tavily_client.search(\n",
    "            query,\n",
    "            max_results=max_results,\n",
    "            include_raw_content=include_raw_content,\n",
    "            topic=topic\n",
    "        )\n",
    "        search_docs.append(result)\n",
    "\n",
    "    return search_docs\n",
    "\n",
    "def summarize_webpage_content(webpage_content: str) -> str:\n",
    "    \"\"\"Summarize webpage content using the configured summarization model.\n",
    "    \n",
    "    Args:\n",
    "        webpage_content: Raw webpage content to summarize\n",
    "        \n",
    "    Returns:\n",
    "        Formatted summary with key excerpts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up structured output model for summarization\n",
    "        structured_model = summarization_model.with_structured_output(Summary)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = structured_model.invoke([\n",
    "            HumanMessage(content=summarize_webpage_prompt.format(\n",
    "                webpage_content=webpage_content, \n",
    "                date=get_today_str()\n",
    "            ))\n",
    "        ])\n",
    "        \n",
    "        # Format summary with clear structure\n",
    "        formatted_summary = (\n",
    "            f\"<summary>\\n{summary.summary}\\n</summary>\\n\\n\"\n",
    "            f\"<key_excerpts>\\n{summary.key_excerpts}\\n</key_excerpts>\"\n",
    "        )\n",
    "        \n",
    "        return formatted_summary\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to summarize webpage: {str(e)}\")\n",
    "        return webpage_content[:1000] + \"...\" if len(webpage_content) > 1000 else webpage_content\n",
    "\n",
    "def deduplicate_search_results(search_results: List[dict]) -> dict:\n",
    "    \"\"\"Deduplicate search results by URL to avoid processing duplicate content.\n",
    "    \n",
    "    Args:\n",
    "        search_results: List of search result dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping URLs to unique results\n",
    "    \"\"\"\n",
    "    unique_results = {}\n",
    "    \n",
    "    for response in search_results:\n",
    "        for result in response['results']:\n",
    "            url = result['url']\n",
    "            if url not in unique_results:\n",
    "                unique_results[url] = result\n",
    "    \n",
    "    return unique_results\n",
    "\n",
    "def process_search_results(unique_results: dict) -> dict:\n",
    "    \"\"\"Process search results by summarizing content where available.\n",
    "    \n",
    "    Args:\n",
    "        unique_results: Dictionary of unique search results\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of processed results with summaries\n",
    "    \"\"\"\n",
    "    summarized_results = {}\n",
    "    \n",
    "    for url, result in unique_results.items():\n",
    "        # Use existing content if no raw content for summarization\n",
    "        if not result.get(\"raw_content\"):\n",
    "            content = result['content']\n",
    "        else:\n",
    "            # Summarize raw content for better processing\n",
    "            content = summarize_webpage_content(result['raw_content'])\n",
    "        \n",
    "        summarized_results[url] = {\n",
    "            'title': result['title'],\n",
    "            'content': content\n",
    "        }\n",
    "    \n",
    "    return summarized_results\n",
    "\n",
    "def format_search_output(summarized_results: dict) -> str:\n",
    "    \"\"\"Format search results into a well-structured string output.\n",
    "    \n",
    "    Args:\n",
    "        summarized_results: Dictionary of processed search results\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string of search results with clear source separation\n",
    "    \"\"\"\n",
    "    if not summarized_results:\n",
    "        return \"No valid search results found. Please try different search queries or use a different search API.\"\n",
    "    \n",
    "    formatted_output = \"Search results: \\n\\n\"\n",
    "    \n",
    "    for i, (url, result) in enumerate(summarized_results.items(), 1):\n",
    "        formatted_output += f\"\\n\\n--- SOURCE {i}: {result['title']} ---\\n\"\n",
    "        formatted_output += f\"URL: {url}\\n\\n\"\n",
    "        formatted_output += f\"SUMMARY:\\n{result['content']}\\n\\n\"\n",
    "        formatted_output += \"-\" * 80 + \"\\n\"\n",
    "    \n",
    "    return formatted_output\n",
    "\n",
    "# ===== RESEARCH TOOLS =====\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def tavily_search(\n",
    "    query: str,\n",
    "    max_results: Annotated[int, InjectedToolArg] = 3,\n",
    "    topic: Annotated[Literal[\"general\", \"news\", \"finance\"], InjectedToolArg] = \"general\",\n",
    ") -> str:\n",
    "    \"\"\"Fetch results from Tavily search API with content summarization.\n",
    "\n",
    "    Args:\n",
    "        query: A single search query to execute\n",
    "        max_results: Maximum number of results to return\n",
    "        topic: Topic to filter results by ('general', 'news', 'finance')\n",
    "    \"\"\"\n",
    "\n",
    "    # \"\"\"Fetch results from Tavily search API with content summarization.\n",
    "\n",
    "    # Args:\n",
    "    #     query: A single search query to execute\n",
    "    #     max_results: Maximum number of results to return\n",
    "    #     topic: Topic to filter results by ('general', 'news', 'finance')\n",
    "\n",
    "    # Returns:\n",
    "    #     Formatted string of search results with summaries\n",
    "    # \"\"\"\n",
    "\n",
    "    # Execute search for single query\n",
    "    search_results = tavily_search_multiple(\n",
    "        [query],  # Convert single query to list for the internal function\n",
    "        max_results=max_results,\n",
    "        topic=topic,\n",
    "        include_raw_content=True,\n",
    "    )\n",
    "\n",
    "    # Deduplicate results by URL to avoid processing duplicate content\n",
    "    unique_results = deduplicate_search_results(search_results)\n",
    "\n",
    "    # Process results with summarization\n",
    "    summarized_results = process_search_results(unique_results)\n",
    "\n",
    "    # Format output for consumption\n",
    "    return format_search_output(summarized_results)\n",
    "\n",
    "@tool(parse_docstring=True)\n",
    "def think_tool(reflection: str) -> str:\n",
    "    \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "    \n",
    "    Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "    \n",
    "    When to use:\n",
    "    - After receiving search results: What key information did I find?\n",
    "    - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    - When assessing research gaps: What specific information am I still missing?\n",
    "    - Before concluding research: Can I provide a complete answer now?\n",
    "    \n",
    "    Reflection should address:\n",
    "    1. Analysis of current findings - What concrete information have I gathered?\n",
    "    2. Gap assessment - What crucial information is still missing?\n",
    "    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    4. Strategic decision - Should I continue searching or provide my answer?\n",
    "    \n",
    "    Args:\n",
    "        reflection: Your detailed reflection on research progress, findings, gaps, and next steps    \n",
    "    \"\"\"\n",
    "\n",
    "    # \"\"\"Tool for strategic reflection on research progress and decision-making.\n",
    "    \n",
    "    # Use this tool after each search to analyze results and plan next steps systematically.\n",
    "    # This creates a deliberate pause in the research workflow for quality decision-making.\n",
    "    \n",
    "    # When to use:\n",
    "    # - After receiving search results: What key information did I find?\n",
    "    # - Before deciding next steps: Do I have enough to answer comprehensively?\n",
    "    # - When assessing research gaps: What specific information am I still missing?\n",
    "    # - Before concluding research: Can I provide a complete answer now?\n",
    "    \n",
    "    # Reflection should address:\n",
    "    # 1. Analysis of current findings - What concrete information have I gathered?\n",
    "    # 2. Gap assessment - What crucial information is still missing?\n",
    "    # 3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?\n",
    "    # 4. Strategic decision - Should I continue searching or provide my answer?\n",
    "    \n",
    "    # Args:\n",
    "    #     reflection: Your detailed reflection on research progress, findings, gaps, and next steps\n",
    "        \n",
    "    # Returns:\n",
    "    #     Confirmation that reflection was recorded for decision-making\n",
    "    # \"\"\"\n",
    "\n",
    "    return f\"Reflection recorded: {reflection}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Agent\n",
    "\n",
    "#### Overview\n",
    "\n",
    "Our research agent perform iterative tool-calling to search for information. \n",
    "\n",
    "The agent follows a [simple yet effective pattern](https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent):\n",
    "\n",
    "1. **LLM Decision Node**: Analyzes the current state and decides whether to make tool calls or provide a final response\n",
    "2. **Tool Execution Node**: Executes search tools when the LLM determines more information is needed\n",
    "3. **Research Compression Node**: Summarizes and compresses research findings for efficient processing\n",
    "4. **Routing Logic**: Determines workflow continuation based on LLM decisions\n",
    "\n",
    "#### Context Engineering Strategy\n",
    "\n",
    "We apply **context engineering** in two places following the principles outlined in [Context Engineering for Agents](https://blog.langchain.com/context-engineering-for-agents/):\n",
    "\n",
    "#### 1. Webpage Content Summarization\n",
    "Raw search results often contain excessive noise (navigation, ads, boilerplate content). Our `summarize_webpage_content()` function:\n",
    "- Uses structured output to extract key information and relevant excerpts\n",
    "- Filters out irrelevant content while preserving factual details\n",
    "- Compresses lengthy articles into focused summaries\n",
    "- Maintains source attribution for credibility\n",
    "\n",
    "#### 2. Research Result Compression  \n",
    "As the agent performs multiple searches, the conversation context grows rapidly. Our `compress_research()` function:\n",
    "- Synthesizes findings from multiple tool calls into cohesive insights\n",
    "- Extracts raw notes for detailed analysis while maintaining compressed summaries\n",
    "- Reduces token usage for subsequent LLM calls\n",
    "- Preserves essential information for report writing\n",
    "\n",
    "This dual-layer context engineering allows the agent to process extensive information efficiently while maintaining high-quality research output.\n",
    "\n",
    "#### 3. Performing Careful Compression\n",
    "[Compression](https://manus.im/blog/Context-Engineering-for-AI-Agents-Lessons-from-Building-Manus) [is risky](https://cognition.ai/blog/dont-build-multi-agents)! We need to be very careful about loosing valuable information. We'll use an LLM for compression with instructions in a system prompt that comes before a potentially long, token-heavy trajectory of multiple tool calls. The long context can cause the compression LLM [to loose sight of the task instructions](https://www.dbreunig.com/2025/06/22/how-contexts-fail-and-how-to-fix-them.html), leading to generic summaries that loose information. So, we reinforce the compression task by adding a `compress_research_human_message` that:\n",
    "- Explicitly restates the original research topic at compression time\n",
    "- Reminds the model to preserve ALL information relevant to the specific question\n",
    "- Emphasizes that comprehensive findings are critical for final report generation\n",
    "- Prevents task drift during the compression phase\n",
    "\n",
    "#### 4. Output Token Management\n",
    "Research compression can generate long outputs. We need to sure that they do not exceed model token limits, which can cause truncated responses that cut off mid-sentence (as seen with \"**Sextant Coffee Ro\" being cut off). As an example, [GPT-4.1 has output limit of up to 33k tokens](https://openai.com/index/gpt-4-1/) and [Claude4 sonnet supports 64k](https://www.anthropic.com/claude/sonnet). \n",
    "\n",
    "Model SDKs / LangChain integrations may cap this (e.g., [to `1024` tokens in the case of Claude](https://github.com/langchain-ai/langchain/blob/master/libs/partners/anthropic/langchain_anthropic/chat_models.py#L1149C5-L1149C15)). Simply ensure that max tokens is set to ensure complete output. This prevents incomplete compression outputs and ensures full research findings are preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/deep_research_from_scratch/research_agent.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/deep_research_from_scratch/research_agent.py\n",
    "\n",
    "\"\"\"Research Agent Implementation.\n",
    "\n",
    "This module implements a research agent that can perform iterative web searches\n",
    "and synthesis to answer complex research questions.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, filter_messages\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from src.deep_research_from_scratch.state_research import ResearcherState, ResearcherOutputState\n",
    "from src.deep_research_from_scratch.utils import tavily_search, get_today_str, think_tool\n",
    "from src.deep_research_from_scratch.prompts import research_agent_prompt, compress_research_system_prompt, compress_research_human_message\n",
    "\n",
    "#-Azure------------------------------------\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "#------------------------------------------\n",
    "\n",
    "# ===== CONFIGURATION =====\n",
    "\n",
    "# Set up tools and model binding\n",
    "tools = [tavily_search, think_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Initialize models\n",
    "\n",
    "#-Azure------------------------------------------------------------------\n",
    "model = AzureChatOpenAI(\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY_US'),  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENA'I_ENDPOINT\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    ")\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "summarization_model = model\n",
    "compress_model = AzureChatOpenAI(\n",
    "    api_key = os.getenv('AZURE_OPENAI_API_KEY_US'),  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENA'I_ENDPOINT\"),\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    max_tokens=32000\n",
    ") \n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "# model = init_chat_model(model=\"openai:gpt-4.1\") # model=\"anthropic:claude-sonnet-4-20250514\"\n",
    "# model_with_tools = model.bind_tools(tools)\n",
    "# summarization_model = init_chat_model(model=\"openai:gpt-4.1-mini\")\n",
    "# compress_model = init_chat_model(model=\"openai:gpt-4.1\", max_tokens=32000) # model=\"anthropic:claude-sonnet-4-20250514\", max_tokens=64000\n",
    "\n",
    "# ===== AGENT NODES =====\n",
    "\n",
    "def llm_call(state: ResearcherState):\n",
    "    \"\"\"Analyze current state and decide on next actions.\n",
    "    \n",
    "    The model analyzes the current conversation state and decides whether to:\n",
    "    1. Call search tools to gather more information\n",
    "    2. Provide a final answer based on gathered information\n",
    "    \n",
    "    Returns updated state with the model's response.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"researcher_messages\": [\n",
    "            model_with_tools.invoke(\n",
    "                [SystemMessage(content=research_agent_prompt)] + state[\"researcher_messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def tool_node(state: ResearcherState):\n",
    "    \"\"\"Execute all tool calls from the previous LLM response.\n",
    "    \n",
    "    Executes all tool calls from the previous LLM responses.\n",
    "    Returns updated state with tool execution results.\n",
    "    \"\"\"\n",
    "    tool_calls = state[\"researcher_messages\"][-1].tool_calls\n",
    " \n",
    "    # Execute all tool calls\n",
    "    observations = []\n",
    "    for tool_call in tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observations.append(tool.invoke(tool_call[\"args\"]))\n",
    "            \n",
    "    # Create tool message outputs\n",
    "    tool_outputs = [\n",
    "        ToolMessage(\n",
    "            content=observation,\n",
    "            name=tool_call[\"name\"],\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        ) for observation, tool_call in zip(observations, tool_calls)\n",
    "    ]\n",
    "    \n",
    "    return {\"researcher_messages\": tool_outputs}\n",
    "\n",
    "def compress_research(state: ResearcherState) -> dict:\n",
    "    \"\"\"Compress research findings into a concise summary.\n",
    "    \n",
    "    Takes all the research messages and tool outputs and creates\n",
    "    a compressed summary suitable for the supervisor's decision-making.\n",
    "    \"\"\"\n",
    "    \n",
    "    system_message = compress_research_system_prompt.format(date=get_today_str())\n",
    "    messages = [SystemMessage(content=system_message)] + state.get(\"researcher_messages\", []) + [HumanMessage(content=compress_research_human_message)]\n",
    "    response = compress_model.invoke(messages)\n",
    "    \n",
    "    # Extract raw notes from tool and AI messages\n",
    "    raw_notes = [\n",
    "        str(m.content) for m in filter_messages(\n",
    "            state[\"researcher_messages\"], \n",
    "            include_types=[\"tool\", \"ai\"]\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"compressed_research\": str(response.content),\n",
    "        \"raw_notes\": [\"\\n\".join(raw_notes)]\n",
    "    }\n",
    "\n",
    "\n",
    "# ===== ROUTING LOGIC =====\n",
    "\n",
    "def should_continue(state: ResearcherState) -> Literal[\"tool_node\", \"compress_research\"]:\n",
    "    \"\"\"Determine whether to continue research or provide final answer.\n",
    "    \n",
    "    Determines whether the agent should continue the research loop or provide\n",
    "    a final answer based on whether the LLM made tool calls.\n",
    "    \n",
    "    Returns:\n",
    "        \"tool_node\": Continue to tool execution\n",
    "        \"compress_research\": Stop and compress research\n",
    "    \"\"\"\n",
    "    messages = state[\"researcher_messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM makes a tool call, continue to tool execution\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    # Otherwise, we have a final answer\n",
    "    return \"compress_research\"\n",
    "\n",
    "\n",
    "# ===== GRAPH CONSTRUCTION =====\n",
    "\n",
    "# Build the agent workflow\n",
    "agent_builder = StateGraph(ResearcherState, output_schema=ResearcherOutputState)\n",
    "\n",
    "# Add nodes to the graph\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "agent_builder.add_node(\"compress_research\", compress_research)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node\": \"tool_node\", # Continue research loop\n",
    "        \"compress_research\": \"compress_research\", # Provide final answer\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\") # Loop back for more research\n",
    "agent_builder.add_edge(\"compress_research\", END)\n",
    "\n",
    "# Compile the agent\n",
    "researcher_agent = agent_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from src.deep_research_from_scratch.research_agent import researcher_agent\n",
    "\n",
    "# Show the agent\n",
    "# display(Image(researcher_agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='api.tavily.com', port=443): Max retries exceeded with url: /search (Caused by NewConnectionError(\"HTTPSConnection(host='api.tavily.com', port=443): Failed to establish a new connection: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connection.py:204\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNewConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connection.py:759\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    758\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connection.py:219\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[32m    220\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    221\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    223\u001b[39m sys.audit(\u001b[33m\"\u001b[39m\u001b[33mhttp.client.connect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m.port)\n",
      "\u001b[31mNewConnectionError\u001b[39m: HTTPSConnection(host='api.tavily.com', port=443): Failed to establish a new connection: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\requests\\adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='api.tavily.com', port=443): Max retries exceeded with url: /search (Caused by NewConnectionError(\"HTTPSConnection(host='api.tavily.com', port=443): Failed to establish a new connection: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Example brief\u001b[39;00m\n\u001b[32m      6\u001b[39m research_brief = \u001b[33m\"\"\"\u001b[39m\u001b[33mHow can I develop a multi-agent application in Python using LangGraph, where sub-agents are responsible for (1)    \u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mRetrieval-Augmented Generation (RAG), (2) querying an SQL database, and (3) performing web search, and a main agent\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mcoordinates these sub-agents to compile all gathered information into a final report? Please include best practices\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[33mlibraries beyond Python and LangGraph have been specified, so consider all relevant options within this scope.   \u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m result = \u001b[43mresearcher_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresearcher_messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mresearch_brief\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m format_messages(result[\u001b[33m'\u001b[39m\u001b[33mresearcher_messages\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langgraph\\pregel\\main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langgraph\\pregel\\main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\deep_research\\src\\deep_research_from_scratch\\research_agent.py:86\u001b[39m, in \u001b[36mtool_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m tool_calls:\n\u001b[32m     85\u001b[39m     tool = tools_by_name[tool_call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]]\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     observations.append(\u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43margs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Create tool message outputs\u001b[39;00m\n\u001b[32m     89\u001b[39m tool_outputs = [\n\u001b[32m     90\u001b[39m     ToolMessage(\n\u001b[32m     91\u001b[39m         content=observation,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     ) \u001b[38;5;28;01mfor\u001b[39;00m observation, tool_call \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(observations, tool_calls)\n\u001b[32m     95\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_core\\tools\\base.py:605\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    599\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    602\u001b[39m     **kwargs: Any,\n\u001b[32m    603\u001b[39m ) -> Any:\n\u001b[32m    604\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_core\\tools\\base.py:932\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    931\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m932\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    933\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    934\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_core\\tools\\base.py:898\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    897\u001b[39m         tool_kwargs |= {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    900\u001b[39m     msg = (\n\u001b[32m    901\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSince response_format=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    902\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33ma two-tuple of the message content and raw tool output is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    903\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected. Instead, generated response is of type: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    904\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(response)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    905\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\langchain_core\\tools\\structured.py:93\u001b[39m, in \u001b[36mStructuredTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m     92\u001b[39m         kwargs[config_param] = config\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mStructuredTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\deep_research\\src\\deep_research_from_scratch\\utils.py:220\u001b[39m, in \u001b[36mtavily_search\u001b[39m\u001b[34m(query, max_results, topic)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fetch results from Tavily search API with content summarization.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m \u001b[33;03m    topic: Topic to filter results by ('general', 'news', 'finance')\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# \"\"\"Fetch results from Tavily search API with content summarization.\u001b[39;00m\n\u001b[32m    209\u001b[39m \n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    218\u001b[39m \n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Execute search for single query\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m search_results = \u001b[43mtavily_search_multiple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Convert single query to list for the internal function\u001b[39;49;00m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude_raw_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[38;5;66;03m# Deduplicate results by URL to avoid processing duplicate content\u001b[39;00m\n\u001b[32m    228\u001b[39m unique_results = deduplicate_search_results(search_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\deep_research\\src\\deep_research_from_scratch\\utils.py:82\u001b[39m, in \u001b[36mtavily_search_multiple\u001b[39m\u001b[34m(search_queries, max_results, topic, include_raw_content)\u001b[39m\n\u001b[32m     80\u001b[39m search_docs = []\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m search_queries:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     result = \u001b[43mtavily_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_raw_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_raw_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopic\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     search_docs.append(result)\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m search_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\tavily\\tavily.py:139\u001b[39m, in \u001b[36mTavilyClient.search\u001b[39m\u001b[34m(self, query, search_depth, topic, time_range, start_date, end_date, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, timeout, country, auto_parameters, include_favicon, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03mCombined search method.\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    138\u001b[39m timeout = \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[32m120\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m response_dict = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m                             \u001b[49m\u001b[43msearch_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mtime_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mdays\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m                             \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m                             \u001b[49m\u001b[43minclude_answer\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_answer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m                             \u001b[49m\u001b[43minclude_raw_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_raw_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m                             \u001b[49m\u001b[43minclude_images\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcountry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mauto_parameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauto_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m                             \u001b[49m\u001b[43minclude_favicon\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_favicon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m                             \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m tavily_results = response_dict.get(\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m    161\u001b[39m response_dict[\u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m] = tavily_results\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\tavily\\tavily.py:89\u001b[39m, in \u001b[36mTavilyClient._search\u001b[39m\u001b[34m(self, query, search_depth, topic, time_range, start_date, end_date, days, max_results, include_domains, exclude_domains, include_answer, include_raw_content, include_images, timeout, country, auto_parameters, include_favicon, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m timeout = \u001b[38;5;28mmin\u001b[39m(timeout, \u001b[32m120\u001b[39m)\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/search\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.Timeout:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\requests\\api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\UserData\\BB033377\\my_projects\\.agents-ud\\Lib\\site-packages\\requests\\adapters.py:677\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    673\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e.reason, _SSLError):\n\u001b[32m    674\u001b[39m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[32m    675\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPSConnectionPool(host='api.tavily.com', port=443): Max retries exceeded with url: /search (Caused by NewConnectionError(\"HTTPSConnection(host='api.tavily.com', port=443): Failed to establish a new connection: [WinError 10013] An attempt was made to access a socket in a way forbidden by its access permissions\"))",
      "During task with name 'tool_node' and id '89d790eb-12b0-b310-d977-81b78e3def47'"
     ]
    }
   ],
   "source": [
    "# Run the agent\n",
    "from utils import format_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Example brief\n",
    "research_brief = \"\"\"How can I develop a multi-agent application in Python using LangGraph, where sub-agents are responsible for (1)    \n",
    "Retrieval-Augmented Generation (RAG), (2) querying an SQL database, and (3) performing web search, and a main agent\n",
    "coordinates these sub-agents to compile all gathered information into a final report? Please include best practices\n",
    "for agent design and orchestration in LangGraph, integration strategies for RAG, SQL, and web search               \n",
    "functionalities, and approaches for compiling and presenting the final report. Aspects such as scalability, error  \n",
    "handling, and security should be considered unless specific constraints are required. No particular sources or     \n",
    "libraries beyond Python and LangGraph have been specified, so consider all relevant options within this scope.   \n",
    "\"\"\"\n",
    "\n",
    "result = researcher_agent.invoke({\"researcher_messages\": [HumanMessage(content=f\"{research_brief}.\")]})\n",
    "format_messages(result['researcher_messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(result['compressed_research'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents use a self-directed trajectory of tool calls to accomplish tasks.\n",
    "\n",
    "Let's think about the ramifications of a poorly tuned tool-calling loop: \n",
    "\n",
    "- Early Termination: The agent decides to stop calling tools when the task is not actually complete. \n",
    "- Prolonged Looping: The agent is never satisfied with the state of its information. \n",
    "\n",
    "The first failure  mode results in the agent collecting insufficient information. In the deep research context, the answer may end up being too shallow. The second failure mode results in the agent utilizing an excessive number of tokens. In the deep research context, irrelevant information may poison the result.\n",
    "\n",
    "We tuned our prompt to avoid these problems. But, let's se up a toy eval dataset that allows us to easily test agent decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, ToolMessage, HumanMessage\n",
    "\n",
    "# Scenario 1: Agent should CONTINUE - has insufficient/irrelevant information + think_tool realizes more research needed\n",
    "messages_should_continue = [\n",
    "    HumanMessage(content=\"What are the top coffee shops in San Francisco based on coffee quality?\"),\n",
    "    AIMessage(\n",
    "        content=\"I'll search for information about top coffee shops in San Francisco focusing on coffee quality.\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"tavily_search\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"coffee shops San Francisco popular locations\"\n",
    "                },\n",
    "                \"id\": \"call_continue_search_001\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    # Tavily Search provides generic cafe information - NOT focused on coffee quality\n",
    "    ToolMessage(\n",
    "        content=\"Search results show popular San Francisco cafes: Blue Bottle Coffee has great wifi and ambiance, Philz Coffee is known for custom blends and friendly service, Café Reveille has multiple locations with good pastries and laptop-friendly spaces. Ritual Coffee offers cozy seating and is popular with students.\",\n",
    "        tool_call_id=\"call_continue_search_001\",\n",
    "    ),\n",
    "    # Agent uses think_tool to reflect and realizes the search results are insufficient\n",
    "    AIMessage(\n",
    "        content=\"Let me use the think_tool to analyze these search results and determine next steps.\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"think_tool\",\n",
    "                \"args\": {\n",
    "                    \"reflection\": \"Analysis of search results: The information I found focuses on general cafe features like wifi, ambiance, custom blends, and seating - but lacks specific coffee quality metrics. The user specifically asked for evaluation based on coffee quality, which would require expert reviews, professional ratings, specialty certifications, or quality-focused customer feedback. I need to search specifically for coffee quality assessments, Coffee Review scores, specialty coffee credentials, or expert evaluations of SF coffee shops. The current information is insufficient to answer the user's question about coffee quality.\"\n",
    "                },\n",
    "                \"id\": \"call_continue_think_001\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    # think_tool response identifies the gap and need for more targeted research\n",
    "    ToolMessage(\n",
    "        content=\"Reflection recorded: Analysis of search results: The information I found focuses on general cafe features like wifi, ambiance, custom blends, and seating - but lacks specific coffee quality metrics. The user specifically asked for evaluation based on coffee quality, which would require expert reviews, professional ratings, specialty certifications, or quality-focused customer feedback. I need to search specifically for coffee quality assessments, Coffee Review scores, specialty coffee credentials, or expert evaluations of SF coffee shops. The current information is insufficient to answer the user's question about coffee quality.\",\n",
    "        tool_call_id=\"call_continue_think_001\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# Scenario 2: Agent should STOP - has comprehensive coffee quality information + think_tool reflection\n",
    "messages_should_stop = [\n",
    "    HumanMessage(content=\"What are the top coffee shops in San Francisco based on coffee quality?\"),\n",
    "    AIMessage(\n",
    "        content=\"I'll research San Francisco coffee shops specifically focusing on coffee quality assessments and expert reviews.\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"tavily_search\",\n",
    "                \"args\": {\n",
    "                    \"query\": \"best coffee quality San Francisco specialty coffee expert reviews Coffee Review ratings 2024\"\n",
    "                },\n",
    "                \"id\": \"call_stop_search_001\", \n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    # This ToolMessage provides COMPREHENSIVE coffee quality information - sufficient to answer the question\n",
    "    ToolMessage(\n",
    "        content=\"\"\"Coffee quality research for San Francisco reveals top establishments based on expert reviews and specialty coffee credentials:\n",
    "\n",
    "**Top Quality-Focused Coffee Shops:**\n",
    "1. **Sightglass Coffee** - Coffee Review score 94/100, known for single-origin beans and precise roasting techniques. Featured in Bon Appétit's \"Best Coffee in America\" list for exceptional coffee quality.\n",
    "\n",
    "2. **Saint Frank Coffee** - Ranked #3 coffee shop in America by The Daily Meal based on coffee quality. 4.3/5 Yelp rating with reviews specifically praising \"exceptional bean quality\" and \"perfect extraction.\"\n",
    "\n",
    "3. **Ritual Coffee Roasters** - Specialty Coffee Association member, direct trade relationships, consistently rated 4.4/5 on Google with reviews highlighting \"superior coffee taste\" and \"expert barista skills.\"\n",
    "\n",
    "4. **Blue Bottle Coffee** - Third-wave coffee pioneer, Coffee Review scores averaging 90+/100, known for rigorous quality control and fresh roasting practices.\n",
    "\n",
    "5. **Four Barrel Coffee** - Local roaster with Coffee Review scores 88-92/100, praised by industry experts for \"exceptional coffee quality\" and innovative brewing methods.\n",
    "\n",
    "**Quality Indicators Found:**\n",
    "- Coffee Review professional scores (88-94/100 range)\n",
    "- Specialty Coffee Association certifications\n",
    "- Direct trade/single-origin sourcing\n",
    "- Expert recognition from food publications\n",
    "- Customer reviews specifically mentioning coffee taste and quality\n",
    "\n",
    "This provides a comprehensive assessment of SF coffee shops based specifically on coffee quality rather than ambiance or service.\"\"\",\n",
    "        tool_call_id=\"call_stop_search_001\",\n",
    "    ),\n",
    "    # Agent now uses think_tool to reflect on the search results\n",
    "    AIMessage(\n",
    "        content=\"Let me reflect on the search results using the think_tool to assess whether I have sufficient information.\",\n",
    "        tool_calls=[\n",
    "            {\n",
    "                \"name\": \"think_tool\",\n",
    "                \"args\": {\n",
    "                    \"reflection\": \"Analysis of search results: I found comprehensive coffee quality information for SF coffee shops including: 1) Professional Coffee Review scores (88-94/100), 2) Expert rankings from reputable publications, 3) Specialty Coffee Association certifications, 4) Specific quality indicators (single-origin, roasting techniques, extraction methods), 5) Five top-rated establishments with detailed quality assessments. This covers all aspects requested: expert reviews, customer ratings, specialty certifications. I have enough information to provide a well-supported ranking based specifically on coffee quality. No additional searches needed - I should provide my comprehensive answer now.\"\n",
    "                },\n",
    "                \"id\": \"call_think_001\",\n",
    "                \"type\": \"tool_call\"\n",
    "            }\n",
    "        ]\n",
    "    ),\n",
    "    # think_tool response confirming reflection was recorded\n",
    "    ToolMessage(\n",
    "        content=\"Reflection recorded: Analysis of search results: I found comprehensive coffee quality information for SF coffee shops including: 1) Professional Coffee Review scores (88-94/100), 2) Expert rankings from reputable publications, 3) Specialty Coffee Association certifications, 4) Specific quality indicators (single-origin, roasting techniques, extraction methods), 5) Five top-rated establishments with detailed quality assessments. This covers all aspects requested: expert reviews, customer ratings, specialty certifications. I have enough information to provide a well-supported ranking based specifically on coffee quality. No additional searches needed - I should provide my comprehensive answer now.\",\n",
    "        tool_call_id=\"call_think_001\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a dataset here that has one example that should continue researching, and another example that should have all of the information necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "\n",
    "langsmith_client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "\n",
    "# Create the dataset\n",
    "dataset_name = \"deep_research_agent_termination\"\n",
    "if not langsmith_client.has_dataset(dataset_name=dataset_name):\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = langsmith_client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"A dataset that evaluates whether a researcher can accurately decide to continue calling tools, or to stop.\",\n",
    "    )\n",
    "    \n",
    "    # Create the examples\n",
    "    langsmith_client.create_examples(\n",
    "        dataset_id=dataset.id,\n",
    "        examples=[\n",
    "            {\n",
    "                \"inputs\": {\"researcher_messages\": messages_should_continue},\n",
    "                \"outputs\": {\"next_step\": \"continue\"},\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\"researcher_messages\": messages_should_stop},\n",
    "                \"outputs\": {\"next_step\": \"stop\"},\n",
    "            },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our evaluator is simple, using a hard-coded heuristic to check if the agent correctly continues or stops.\n",
    "\n",
    "We'll pass in the list of messages above.\n",
    "\n",
    "We'll assess the decision made by the agent: \n",
    "\n",
    "* `stop` - No tool calls means the agent should stop\n",
    "* `continue` - Tool calls means the agent should continue\n",
    "\n",
    "We'll then compare this decision to the reference expected decision `next_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def evaluate_next_step(outputs: dict, reference_outputs:dict):\n",
    "    tool_calls = outputs[\"researcher_messages\"][-1].tool_calls\n",
    "    made_tool_call = len(tool_calls) > 0\n",
    "    return {\n",
    "        \"key\": \"correct_next_step\",\n",
    "        \"score\": made_tool_call == (reference_outputs[\"next_step\"] == \"continue\")\n",
    "    }\n",
    "\n",
    "def target_func(inputs: dict):\n",
    "    config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "    result = researcher_agent.nodes[\"llm_call\"].invoke(inputs, config=config)\n",
    "    return result\n",
    "\n",
    "langsmith_client.evaluate(\n",
    "    target_func,\n",
    "    data=dataset_name,\n",
    "    evaluators=[evaluate_next_step],\n",
    "    experiment_prefix=\"Researcher Iteration\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in one example, the search tool doesn't return enough information, so we submit another search via tool call.\n",
    "\n",
    "In the other example, we get all three top restaurants back, and so we decide to conclude.\n",
    "\n",
    "One neat trick here - we were able to test a single node of our graph by calling `agent.nodes[\"node_name].invoke`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".agents-ud (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
