{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88007b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "parent_dir = os.path.abspath(\"./..\")\n",
    "sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eeb544",
   "metadata": {},
   "source": [
    "# Research Agent with MCP\n",
    "\n",
    "*The goal of research is to gather the context requested by the research brief.*\n",
    "\n",
    "\n",
    "We built an [agent](https://langchain-ai.github.io/langgraph/tutorials/workflows/#agent) that will a custom search tool. But, we can also use the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/specification/2025-06-18/architecture) as another way to access tools! MCP servers provide a standard protocol for accessing tools. [LangChain MCP Adapters](https://github.com/langchain-ai/langchain-mcp-adapters) provide a seamless bridge between the Model Context Protocol (MCP) and LangChain/LangGraph ecosystems. This library enables compatibility between MCP servers and our LangGraph agent. \n",
    "\n",
    "Let's pick one example MCP server and see how it works! The [Filesystem MCP Server](https://github.com/modelcontextprotocol/servers/tree/main/src/filesystem) provides secure, controlled access to local file systems with granular permission management.\n",
    "\n",
    "**What It Provides:**\n",
    "- **File operations** with strict access control\n",
    "- **Directory management** with dynamic permissions\n",
    "- **Search capabilities** across allowed directories\n",
    "- **Metadata access** for files and directories\n",
    "\n",
    "**Available Tools:**\n",
    "- **File Operations**: `read_file`, `write_file`, `edit_file`, `read_multiple_files`\n",
    "- **Directory Management**: `create_directory`, `list_directory`, `move_file`\n",
    "- **Search & Discovery**: `search_files`, `get_file_info`, `list_allowed_directories`\n",
    "\n",
    "### Prompt\n",
    "\n",
    "First, we'll define a prompt that instructs our agent to use available search tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_prompt\n",
    "from src.deep_research_from_scratch.prompts import research_agent_prompt_with_mcp\n",
    "show_prompt(research_agent_prompt_with_mcp, \"Research Agent Instructions MCP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e916b5",
   "metadata": {},
   "source": [
    "### Research Tool\n",
    "\n",
    "Now, we'll use the `Filesystem MCP Server` to access research tools.\n",
    "\n",
    "When using MCP tools with LangChain, you must use async methods because the MCP protocol is inherently asynchronous:\n",
    "- Server communication uses async JSON-RPC over stdio/http\n",
    "- Tool invocations involve network/IPC calls that can be slow\n",
    "- Async enables non-blocking, concurrent operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d47dc5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".agents-ud (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
